{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Real-time Text Classification with BERT + Self-Attention using TorchServe\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook deploys a fine-tuned BERT-based multi-class text classification model for real-time inference using **TorchServe**. The model predicts topics from the 20 Newsgroups dataset across 20 categories, supporting sentence-level classification in production environments via HTTP REST API.\n",
        "\n",
        "## Model Architecture\n",
        "\n",
        "* **Base**: [`bert-base-uncased`](https://huggingface.co/bert-base-uncased)\n",
        "* **Custom Head**:\n",
        "\n",
        "  * Self-Attention Pooling Layer\n",
        "  * Dropout: 0.5\n",
        "  * Fully Connected Layer\n",
        "  * Label Smoothing CrossEntropy Loss (`smoothing=0.1`)\n",
        "* **Output**: 20-class logits over 20 Newsgroups labels\n",
        "\n",
        "## Deployment Setup\n",
        "\n",
        "### Frameworks & Tools\n",
        "\n",
        "* PyTorch\n",
        "* TorchServe\n",
        "* HuggingFace Transformers\n",
        "* Ngrok (for public URL access)\n",
        "* Python `requests` (for client-side demo)\n",
        "\n",
        "### TorchServe Artifacts\n",
        "\n",
        "* `bert_sa_model.mar`: Model archive including:\n",
        "\n",
        "  * Serialized model weights: `bert_final_sa_model_state_dict.pt`\n",
        "  * Custom handler: `handler.py`\n",
        "  * Index to label mapping: `id2label`\n",
        "* `config.properties`: Port setup and model load settings"
      ],
      "metadata": {
        "id": "tGql8_jPPvsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Serving Steps："
      ],
      "metadata": {
        "id": "dyqs5KI1elug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment Setup\n",
        "Mount Google Drive and install required packages."
      ],
      "metadata": {
        "id": "u87s_5TIWKzg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VANhxhVtgXkm",
        "outputId": "572bf749-37fb-45b7-f1f4-62b096f8e055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-model-archiver"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIyOVWapgx4h",
        "outputId": "3613824c-6bb5-4727-f0bc-c0df49227bd3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-model-archiver\n",
            "  Downloading torch_model_archiver-0.12.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting enum-compat (from torch-model-archiver)\n",
            "  Downloading enum_compat-0.0.3-py3-none-any.whl.metadata (954 bytes)\n",
            "Downloading torch_model_archiver-0.12.0-py3-none-any.whl (16 kB)\n",
            "Downloading enum_compat-0.0.3-py3-none-any.whl (1.3 kB)\n",
            "Installing collected packages: enum-compat, torch-model-archiver\n",
            "Successfully installed enum-compat-0.0.3 torch-model-archiver-0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchserve torch-model-archiver transformers nltk evaluate scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3TT03chbFPQ",
        "outputId": "004be04f-08f5-4a52-d3c5-c5bbae595dad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchserve\n",
            "  Downloading torchserve-0.12.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: torch-model-archiver in /usr/local/lib/python3.11/dist-packages (0.12.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from torchserve) (11.2.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from torchserve) (5.9.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from torchserve) (24.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from torchserve) (0.45.1)\n",
            "Requirement already satisfied: enum-compat in /usr/local/lib/python3.11/dist-packages (from torch-model-archiver) (0.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Collecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading torchserve-0.12.0-py3-none-any.whl (42.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, torchserve, fsspec, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 evaluate-0.4.3 fsspec-2025.3.0 multiprocess-0.70.16 torchserve-0.12.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r /content/drive/MyDrive/bert_final_sa_model/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izxM_08s2hxB",
        "outputId": "8275d6a2-a104-41f3-8f75-f3897767cfe9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2)) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers>=4.39 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 3)) (4.51.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 4)) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 5)) (2.2.2)\n",
            "Requirement already satisfied: nltk>=3.5 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 8)) (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2)) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2)) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 3)) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 3)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 3)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 3)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 3)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 4)) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 4)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 5)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 5)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 5)) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.5->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 8)) (8.1.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 5)) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 2)) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.39->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.39->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.39->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 3)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.39->-r /content/drive/MyDrive/bert_final_sa_model/requirements.txt (line 3)) (2025.4.26)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Model Archiving\n",
        "Use torch-model-archiver to package the trained model."
      ],
      "metadata": {
        "id": "9Bbye2HAO6W7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Set up path ===\n",
        "MODEL_DIR = \"/content/drive/MyDrive/bert_final_sa_model\"\n",
        "TMP_DIR = \"/tmp/bert_mar_dir\"\n",
        "EXPORT_PATH = f\"{MODEL_DIR}/model_store\"\n",
        "MODEL_NAME = \"bert_sa_model\"\n",
        "\n",
        "#!cd {TMP_DIR} && zip -r nltk_data.zip nltk_data"
      ],
      "metadata": {
        "id": "GvcJM2BcO2dk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Package model"
      ],
      "metadata": {
        "id": "aYZiv1nFeLPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!torch-model-archiver \\\n",
        "  --model-name {MODEL_NAME} \\\n",
        "  --version 1.0 \\\n",
        "  --serialized-file {MODEL_DIR}/bert_final_sa_model_state_dict.pt \\\n",
        "  --handler {MODEL_DIR}/handler.py \\\n",
        "  --extra-files \"{MODEL_DIR}/model.py,{MODEL_DIR}/requirements.txt\" \\\n",
        "  --requirements-file {MODEL_DIR}/requirements.txt \\\n",
        "  --export-path {EXPORT_PATH} \\\n",
        "  --force"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1086UYUPZl49",
        "outputId": "6953b54b-7d9f-4c31-fe1e-ea26d64405cc"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING - Overwriting /content/drive/MyDrive/bert_final_sa_model/model_store/bert_sa_model.mar ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -l /content/drive/MyDrive/bert_final_sa_model/model_store/bert_sa_model.mar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPLPCR5MQfiR",
        "outputId": "93d9d5be-3a73-4a92-d9df-855bddef1d36"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/bert_final_sa_model/model_store/bert_sa_model.mar\n",
            "  Length      Date    Time    Name\n",
            "---------  ---------- -----   ----\n",
            "      444  2025-05-04 06:47   requirements.txt\n",
            "     1983  2025-05-03 19:53   model.py\n",
            "     4844  2025-05-04 06:47   handler.py\n",
            "438476617  2025-05-04 06:47   bert_final_sa_model_state_dict.pt\n",
            "      304  2025-05-04 06:47   MAR-INF/MANIFEST.json\n",
            "---------                     -------\n",
            "438484192                     5 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -l /content/drive/MyDrive/bert_final_sa_model/model_store/bert_sa_model.mar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmbV-KqOJvNE",
        "outputId": "e6872867-00af-487e-a6cf-1270808dc113"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/bert_final_sa_model/model_store/bert_sa_model.mar\n",
            "  Length      Date    Time    Name\n",
            "---------  ---------- -----   ----\n",
            "      444  2025-05-04 03:46   requirements.txt\n",
            "     1983  2025-05-04 03:42   model.py\n",
            "     4050  2025-05-04 03:46   handler.py\n",
            " 73941280  2025-05-04 03:45   nltk_data.zip\n",
            "438476617  2025-05-04 03:46   bert_final_sa_model_state_dict.pt\n",
            "      304  2025-05-04 03:46   MAR-INF/MANIFEST.json\n",
            "---------                     -------\n",
            "512424678                     6 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/drive/MyDrive/bert_final_sa_model/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bke5a_Aj8fze",
        "outputId": "33512d42-f81f-48db-f575-9dfa3955aec9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# === Core ML dependencies ===\n",
            "torch>=2.0\n",
            "transformers>=4.39\n",
            "scikit-learn\n",
            "pandas\n",
            "\n",
            "# === NLP utilities ===\n",
            "nltk>=3.5\n",
            "\n",
            "# === Optional: tokenizer dependencies\n",
            "# sentencepiece     # If you use T5, ALBERT, etc.\n",
            "# protobuf          # For exporting/loading models\n",
            "# regex             # Often required for advanced tokenization\n",
            "\n",
            "# === NLTK data files (handled separately)\n",
            "# nltk.download('punkt')\n",
            "# nltk.download('stopwords')\n",
            "# nltk.download('wordnet')"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. TorchServe Deployment\n",
        "Start the TorchServe model server and register the model."
      ],
      "metadata": {
        "id": "H2uzU0U0c8pN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.listdir(\"/content/drive/MyDrive/bert_final_sa_model/model_store\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5LvNYf1hUla",
        "outputId": "f6fe07fc-e5c6-4470-eaee-3575ed70bfb6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bert_sa_model.mar']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!torchserve --stop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSwwUVXokv2D",
        "outputId": "aeda9b62-6010-426d-c996-f40a21513f71"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TorchServe has stopped.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"\" > config.properties"
      ],
      "metadata": {
        "id": "waN6G50rjv07"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Torch Server Start"
      ],
      "metadata": {
        "id": "ry9Pdwqmdi3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_path = \"/content/config.properties\"\n",
        "\n",
        "with open(config_path, \"w\") as f:\n",
        "    f.write(\"\"\"\\\n",
        "inference_address=http://0.0.0.0:8085\n",
        "management_address=http://0.0.0.0:8086\n",
        "metrics_address=http://0.0.0.0:8087\n",
        "model_store=/content/drive/MyDrive/bert_final_sa_model/model_store\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "iXJ946_ZlwQT"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /usr/local/lib/python3.11/dist-packages/pathlib.py"
      ],
      "metadata": {
        "id": "mT-GE3x6ZEUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a1280b2-354c-4a3b-db28-67b3271320c6"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/usr/local/lib/python3.11/dist-packages/pathlib.py': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Inference and Testing\n",
        "Use pyngrok and requests to send HTTP requests to the deployed model."
      ],
      "metadata": {
        "id": "YMbQFkRTdKsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Launch TorchServe"
      ],
      "metadata": {
        "id": "8uK4wA7feTlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!torchserve --stop\n",
        "!rm -rf /tmp/.ts.sock* logs/*\n",
        "\n",
        "!nohup torchserve \\\n",
        "  --start \\\n",
        "  --ts-config /content/config.properties \\\n",
        "  --models bert_sa_model=bert_sa_model.mar \\\n",
        "  --ncs > server.log 2>&1 &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzWc3xuKlj5Q",
        "outputId": "13f3fadd-b445-4c3e-97a9-44512f41224b"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TorchServe is not currently running.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load logs\n",
        "!tail -n 50 server.log"
      ],
      "metadata": {
        "id": "cDjlKYhgxItM"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/key_file.json\", \"r\") as f:\n",
        "    key_data = json.load(f)\n",
        "print(key_data)\n",
        "\n",
        "# Create token\n",
        "access_token = key_data[\"inference\"][\"key\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJxWRxV-yOvR",
        "outputId": "626e5a50-5c11-44d8-ba59-3c98c1e32347"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'management': {'key': '4WzFHRnp', 'expiration time': '2025-05-04T07:48:01.769608Z'}, 'inference': {'key': '0RDAAoNF', 'expiration time': '2025-05-04T07:48:01.769565Z'}, 'API': {'key': 'phb0QRbc'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyngrok"
      ],
      "metadata": {
        "id": "E9dBvDuRVOst"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forcefully close all tunnels in the current session\n",
        "from pyngrok import ngrok\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "bmY-LQpzf9oX"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken \"2wbaoW7f35b7ooh3gGpHkLAX141_3m8iKrJFZ23fvDX5vVjfB\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJxv1CFtXqUN",
        "outputId": "2056421f-aeb1-42e8-c2f7-a22172214cb0"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tunnel with ngrok"
      ],
      "metadata": {
        "id": "d4qBfFtHebJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# === Open public endpoint ===\n",
        "public_url = ngrok.connect(8085).public_url\n",
        "print(\"Your inference URL is:\", public_url)\n",
        "\n",
        "# === Read token ===\n",
        "with open(\"/content/key_file.json\") as f:\n",
        "    key_data = json.load(f)\n",
        "\n",
        "access_token = key_data[\"inference\"][\"key\"]\n",
        "\n",
        "# === Set request headers and payload ===\n",
        "headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
        "payload = {\"text\": \"this is a sample sentence\"}\n",
        "\n",
        "# === Send inference request ===\n",
        "response = requests.post(\n",
        "    f\"{public_url}/predictions/bert_sa_model\",\n",
        "    headers=headers,\n",
        "    json=[payload]  # ← make sure to wrap payload in a list!\n",
        ")\n",
        "\n",
        "# === Print response ===\n",
        "print(\"Response:\", response.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk7VJ8d8ylx3",
        "outputId": "d3b02ef0-c092-43dc-f4d8-5fb601f8a44a"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your inference URL is: https://e6f7-35-197-26-122.ngrok-free.app\n",
            "Response: {'predicted_label_id': 18, 'predicted_label': 'talk.politics.misc'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load logs\n",
        "!tail -n 50 server.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weqalZqTYpVO",
        "outputId": "d73f5c89-8b6e-45ec-d0f5-486b96b3ce52"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-04T06:48:25,539 [INFO ] W-9001-bert_sa_model_1.0-stdout MODEL_LOG - Initializing model and tokenizer...\n",
            "2025-05-04T06:48:25,549 [INFO ] W-9000-bert_sa_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled\n",
            "2025-05-04T06:48:25,550 [INFO ] W-9000-bert_sa_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime\n",
            "2025-05-04T06:48:25,550 [INFO ] W-9000-bert_sa_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled\n",
            "2025-05-04T06:48:25,551 [INFO ] W-9000-bert_sa_model_1.0-stdout MODEL_LOG - Handler script loaded\n",
            "2025-05-04T06:48:25,552 [INFO ] W-9000-bert_sa_model_1.0-stdout MODEL_LOG - Initializing model and tokenizer...\n",
            "2025-05-04T06:48:28,377 [INFO ] W-9000-bert_sa_model_1.0-stdout MODEL_LOG - NumExpr defaulting to 2 threads.\n",
            "2025-05-04T06:48:28,378 [INFO ] W-9001-bert_sa_model_1.0-stdout MODEL_LOG - NumExpr defaulting to 2 threads.\n",
            "2025-05-04T06:48:32,238 [WARN ] W-9001-bert_sa_model_1.0-stderr MODEL_LOG - 2025-05-04 06:48:32.237763: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-04T06:48:32,300 [WARN ] W-9000-bert_sa_model_1.0-stderr MODEL_LOG - 2025-05-04 06:48:32.299818: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-05-04T06:48:32,365 [WARN ] W-9001-bert_sa_model_1.0-stderr MODEL_LOG - WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "2025-05-04T06:48:32,366 [WARN ] W-9001-bert_sa_model_1.0-stderr MODEL_LOG - E0000 00:00:1746341312.360308   13527 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-04T06:48:32,393 [WARN ] W-9001-bert_sa_model_1.0-stderr MODEL_LOG - E0000 00:00:1746341312.393330   13527 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-04T06:48:32,404 [WARN ] W-9000-bert_sa_model_1.0-stderr MODEL_LOG - WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "2025-05-04T06:48:32,412 [WARN ] W-9000-bert_sa_model_1.0-stderr MODEL_LOG - E0000 00:00:1746341312.402775   13526 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-05-04T06:48:32,451 [WARN ] W-9000-bert_sa_model_1.0-stderr MODEL_LOG - E0000 00:00:1746341312.451142   13526 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-04T06:48:32,522 [WARN ] W-9001-bert_sa_model_1.0-stderr MODEL_LOG - 2025-05-04 06:48:32.522481: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "2025-05-04T06:48:32,523 [WARN ] W-9001-bert_sa_model_1.0-stderr MODEL_LOG - To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-04T06:48:32,583 [WARN ] W-9000-bert_sa_model_1.0-stderr MODEL_LOG - 2025-05-04 06:48:32.582546: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "2025-05-04T06:48:32,583 [WARN ] W-9000-bert_sa_model_1.0-stderr MODEL_LOG - To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-04T06:48:44,503 [INFO ] W-9001-bert_sa_model_1.0-stdout MODEL_LOG - Model initialized successfully.\n",
            "2025-05-04T06:48:44,506 [INFO ] W-9001-bert_sa_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 20967\n",
            "2025-05-04T06:48:44,506 [INFO ] W-9000-bert_sa_model_1.0-stdout MODEL_LOG - Model initialized successfully.\n",
            "2025-05-04T06:48:44,507 [DEBUG] W-9001-bert_sa_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-bert_sa_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED\n",
            "2025-05-04T06:48:44,507 [INFO ] W-9001-bert_sa_model_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:28638.0|#WorkerName:W-9001-bert_sa_model_1.0,Level:Host|#hostname:811494540ff8,timestamp:1746341324\n",
            "2025-05-04T06:48:44,508 [INFO ] W-9001-bert_sa_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:13.0|#Level:Host|#hostname:811494540ff8,timestamp:1746341324\n",
            "2025-05-04T06:48:44,509 [INFO ] W-9000-bert_sa_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 20973\n",
            "2025-05-04T06:48:44,509 [DEBUG] W-9000-bert_sa_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-bert_sa_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED\n",
            "2025-05-04T06:48:44,509 [INFO ] W-9000-bert_sa_model_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:28644.0|#WorkerName:W-9000-bert_sa_model_1.0,Level:Host|#hostname:811494540ff8,timestamp:1746341324\n",
            "2025-05-04T06:48:44,510 [INFO ] W-9000-bert_sa_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:6.0|#Level:Host|#hostname:811494540ff8,timestamp:1746341324\n",
            "2025-05-04T06:48:44,511 [DEBUG] W-9001-bert_sa_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1746341324510\n",
            "2025-05-04T06:48:44,513 [INFO ] W-9001-bert_sa_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1746341324513\n",
            "2025-05-04T06:48:44,517 [INFO ] W-9001-bert_sa_model_1.0-stdout MODEL_LOG - Backend received inference at: 1746341324\n",
            "2025-05-04T06:48:44,519 [INFO ] W-9001-bert_sa_model_1.0-stdout MODEL_LOG - Preprocessing input: [{'body': [{'text': 'this is a sample sentence'}]}]\n",
            "2025-05-04T06:48:44,522 [INFO ] W-9001-bert_sa_model_1.0-stdout MODEL_LOG - Running inference...\n",
            "2025-05-04T06:48:46,196 [INFO ] W-9001-bert_sa_model_1.0-stdout MODEL_LOG - Prediction: 18\n",
            "2025-05-04T06:48:46,197 [INFO ] W-9001-bert_sa_model_1.0-stdout MODEL_LOG - Postprocessed label: talk.politics.misc\n",
            "2025-05-04T06:48:46,197 [INFO ] W-9001-bert_sa_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:1676.8|#ModelName:bert_sa_model,Level:Model|#type:GAUGE|#hostname:811494540ff8,1746341326,a99a7dc1-1f73-4245-aa38-7d9f3e257245, pattern=[METRICS]\n",
            "2025-05-04T06:48:46,198 [INFO ] W-9001-bert_sa_model_1.0 org.pytorch.serve.wlm.BatchAggregator - Sending response for jobId a99a7dc1-1f73-4245-aa38-7d9f3e257245\n",
            "2025-05-04T06:48:46,199 [INFO ] W-9001-bert_sa_model_1.0-stdout MODEL_METRICS - HandlerTime.ms:1676.8|#ModelName:bert_sa_model,Level:Model|#hostname:811494540ff8,requestID:a99a7dc1-1f73-4245-aa38-7d9f3e257245,timestamp:1746341326\n",
            "2025-05-04T06:48:46,199 [INFO ] W-9001-bert_sa_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:1677.16|#ModelName:bert_sa_model,Level:Model|#type:GAUGE|#hostname:811494540ff8,1746341326,a99a7dc1-1f73-4245-aa38-7d9f3e257245, pattern=[METRICS]\n",
            "2025-05-04T06:48:46,200 [INFO ] W-9001-bert_sa_model_1.0-stdout MODEL_METRICS - PredictionTime.ms:1677.16|#ModelName:bert_sa_model,Level:Model|#hostname:811494540ff8,requestID:a99a7dc1-1f73-4245-aa38-7d9f3e257245,timestamp:1746341326\n",
            "2025-05-04T06:48:46,201 [INFO ] W-9001-bert_sa_model_1.0 ACCESS_LOG - /127.0.0.1:56382 \"POST /predictions/bert_sa_model HTTP/1.1\" 200 24974\n",
            "2025-05-04T06:48:46,202 [INFO ] W-9001-bert_sa_model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:811494540ff8,timestamp:1746341326\n",
            "2025-05-04T06:48:46,206 [INFO ] W-9001-bert_sa_model_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:2.4920235346E7|#model_name:bert_sa_model,model_version:default|#hostname:811494540ff8,timestamp:1746341326\n",
            "2025-05-04T06:48:46,207 [INFO ] W-9001-bert_sa_model_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:2.3232075696E7|#model_name:bert_sa_model,model_version:default|#hostname:811494540ff8,timestamp:1746341326\n",
            "2025-05-04T06:48:46,207 [DEBUG] W-9001-bert_sa_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 23232075696, Backend time ns: 1696875726\n",
            "2025-05-04T06:48:46,208 [INFO ] W-9001-bert_sa_model_1.0 TS_METRICS - QueueTime.Milliseconds:23232.0|#Level:Host|#hostname:811494540ff8,timestamp:1746341326\n",
            "2025-05-04T06:48:46,208 [INFO ] W-9001-bert_sa_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1685\n",
            "2025-05-04T06:48:46,208 [INFO ] W-9001-bert_sa_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:13.0|#Level:Host|#hostname:811494540ff8,timestamp:1746341326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### More examples"
      ],
      "metadata": {
        "id": "_ruistmBdh1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# === Start public endpoint ===\n",
        "public_url = ngrok.connect(8085).public_url\n",
        "print(\"Your inference URL is:\", public_url)\n",
        "\n",
        "# === Read access token ===\n",
        "with open(\"/content/key_file.json\") as f:\n",
        "    key_data = json.load(f)\n",
        "access_token = key_data[\"inference\"][\"key\"]\n",
        "\n",
        "# === Set request headers ===\n",
        "headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
        "\n",
        "# === Example input texts ===\n",
        "examples = [\n",
        "    \"The government passed a new bill regulating gun ownership across several states.\",\n",
        "    \"I just installed Ubuntu on my machine and it's much faster than Windows!\",\n",
        "    \"NASA just released new photos from the Hubble telescope showing distant galaxies.\",\n",
        "    \"My Honda Civic has been making a strange noise when I turn left. Any advice?\",\n",
        "    \"Christianity teaches love and forgiveness. What do you think about modern interpretations?\",\n",
        "    \"The Boston Red Sox had an incredible comeback game last night!\",\n",
        "    \"Selling used MacBook Pro 2020 in good condition. DM for details.\",\n",
        "    \"Quantum cryptography could change the way we secure our data.\",\n",
        "    \"I'm looking for help with programming OpenGL in C++. Any good resources?\",\n",
        "    \"Can anyone explain the difference between Sunni and Shia Islam?\"\n",
        "]\n",
        "\n",
        "# === Send requests for each input text ===\n",
        "for text in examples:\n",
        "    payload = [{\"text\": text}]\n",
        "    response = requests.post(\n",
        "        f\"{public_url}/predictions/bert_sa_model\",\n",
        "        headers=headers,\n",
        "        json=payload\n",
        "    )\n",
        "    print(f\"Input: {text}\")\n",
        "    print(\"Response:\", response.json())\n",
        "    print(\"=\" * 60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiDWRDBNI29a",
        "outputId": "6dbfcbe0-b48a-4bf9-9efd-428ad0e74c8c"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your inference URL is: https://65d0-35-197-26-122.ngrok-free.app\n",
            "Input: The government passed a new bill regulating gun ownership across several states.\n",
            "Response: {'predicted_label_id': 16, 'predicted_label': 'talk.politics.guns'}\n",
            "============================================================\n",
            "Input: I just installed Ubuntu on my machine and it's much faster than Windows!\n",
            "Response: {'predicted_label_id': 2, 'predicted_label': 'comp.os.ms-windows.misc'}\n",
            "============================================================\n",
            "Input: NASA just released new photos from the Hubble telescope showing distant galaxies.\n",
            "Response: {'predicted_label_id': 14, 'predicted_label': 'sci.space'}\n",
            "============================================================\n",
            "Input: My Honda Civic has been making a strange noise when I turn left. Any advice?\n",
            "Response: {'predicted_label_id': 7, 'predicted_label': 'rec.autos'}\n",
            "============================================================\n",
            "Input: Christianity teaches love and forgiveness. What do you think about modern interpretations?\n",
            "Response: {'predicted_label_id': 15, 'predicted_label': 'soc.religion.christian'}\n",
            "============================================================\n",
            "Input: The Boston Red Sox had an incredible comeback game last night!\n",
            "Response: {'predicted_label_id': 9, 'predicted_label': 'rec.sport.baseball'}\n",
            "============================================================\n",
            "Input: Selling used MacBook Pro 2020 in good condition. DM for details.\n",
            "Response: {'predicted_label_id': 6, 'predicted_label': 'misc.forsale'}\n",
            "============================================================\n",
            "Input: Quantum cryptography could change the way we secure our data.\n",
            "Response: {'predicted_label_id': 11, 'predicted_label': 'sci.crypt'}\n",
            "============================================================\n",
            "Input: I'm looking for help with programming OpenGL in C++. Any good resources?\n",
            "Response: {'predicted_label_id': 5, 'predicted_label': 'comp.windows.x'}\n",
            "============================================================\n",
            "Input: Can anyone explain the difference between Sunni and Shia Islam?\n",
            "Response: {'predicted_label_id': 0, 'predicted_label': 'alt.atheism'}\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference Example and Prediction Analysis\n",
        "\n",
        "### Strengths\n",
        "\n",
        "* **Semantic Understanding**: Recognizes scientific, religious, technical, and political contexts effectively.\n",
        "* **Label Alignment**: Most predictions match expected 20 Newsgroups categories well.\n",
        "* **Deployment Reliability**: Consistent and correctly structured API responses show successful integration with TorchServe.\n",
        "\n",
        "### Observation\n",
        "\n",
        "* Some ambiguous inputs (e.g., religious comparisons) may be assigned to controversial categories like `alt.atheism`. This reflects the label distribution in the original dataset rather than a model error.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This project demonstrates a full **MLOps-compatible real-time text classification pipeline** using:\n",
        "\n",
        "* A fine-tuned BERT model with a custom attention-based classification head,\n",
        "* TorchServe-based scalable deployment,\n",
        "* Token-based authenticated HTTP requests,\n",
        "* Structured JSON predictions with high semantic alignment.\n"
      ],
      "metadata": {
        "id": "G39SqF0IP384"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "52QuMZhvL3Bc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}